version: '3.8'

services:
  qdrant:
    image: qdrant/qdrant
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage
    networks:
      - app_network

  postgres:
    image: postgres:14
    ports:
      - "${POSTGRES_PORT}:5432"
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: call_center
    env_file: .env
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - app_network

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    networks:
      - app_network

  langchain:
    build:
      context: ./langchain
    volumes:
      - ./documentation:/app/documentation
    depends_on:
      - qdrant
    networks:
      - app_network

  orchestrator:
    build: ./orchestrator
    environment:
      PORT: 300
      POSTGRES_HOST: postgres
      POSTGRES_USER: postgres
      OLLAMA_HOST: ai-agent
      OLLAMA_MODEL: snowflake-arctic-embed2:568m
      OLLAMA_CHAT_MODEL: qwen3:1.7b
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: call_center
      QDRANT_HOST: qdrant
      REDIS_HOST: redis
      REDIS_PORT: 6379
      CONVERSATION_TTL: 86400
    ports:
      - "3001:3000"
    depends_on:
      - postgres
      - qdrant
      - ai-agent
      - redis
    networks:
      - app_network

  ui:
    build:
      context: ./ui
    ports:
      - "8080:80"
    networks:
      - app_network

  whisper:
    image: onerahmet/openai-whisper-asr-webservice:latest-gpu
    environment:
      - ASR_MODEL=large-v3
      - ASR_ENGINE=openai_whisper
    ports:
      - "9000:9000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - app_network

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11435:11434"
    volumes:
      - ollama_data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - app_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 10s
      timeout: 5s
      retries: 5

  ai-agent:
    image: ollama/ollama:latest
    ports:
      - "11436:11434"
    volumes:
      - qwen3_models:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - app_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 10s
      timeout: 5s
      retries: 5

  # New service for Generic Data Validation Framework
  validation-framework:
    build:
      context: ./validation-framework
    ports:
      - "3003:3001" # Main validation API
      - "3004:3002"  # Rule management API
    volumes:
      - ./validation-framework/rules:/app/rules
      - ./validation-framework/configs:/app/configs
    networks:
      - app_network
    depends_on:
      - redis  # If we need Redis for caching rules or results

  # New service for Validation Rule Management UI
  validation-ui:
    build:
      context: ./validation-framework
      dockerfile: Dockerfile.ui
    ports:
      - "3005:3005"  # UI server port
    environment:
      - NODE_ENV=production
    networks:
      - app_network
    depends_on:
      - validation-framework
    command: ["node", "ui-server.js"]

networks:
  app_network:
    driver: bridge

volumes:
  qdrant_data:
  postgres_data:
  ollama_data:
  qwen3_models:
  redis_data: